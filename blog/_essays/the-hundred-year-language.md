---
layout: essay
title: "The Hundred-Year Language"
category: Mindset
difficulty: 중급
year: 
url_original: "https://paulgraham.com/hundred.html"
---

## 요약 (Summary)

🎯 100년 후에도 쓰일 언어는 무엇일까요?

✨ 핵심 내용 요약

Paul Graham이 상상합니다: 100년 후 프로그래머들은 무슨 언어를 쓸까요?

Graham의 예측: (1) 더 간결할 것입니다 - 적은 코드로 더 많은 것을 할 수 있습니다, (2) 더 높은 수준의 추상화 - 세부 사항은 언어가 처리합니다, (3) 더 많은 라이브러리 - 바퀴를 재발명하지 않습니다. 흥미로운 점: 지금도 그런 언어가 있습니다 (Python, JavaScript). 하지만 대부분 회사는 여전히 Java, C++을 씁니다. 왜? 관성(inertia) 때문입니다. 한국 스타트업에게 조언: 100년 후를 기다리지 마세요. 지금 가장 생산적인 언어를 쓰세요.

**핵심 포인트**
• 미래 언어는 더 간결하고 추상적입니다
• 그런 언어는 이미 존재합니다
• 관성을 버리고 생산적인 도구를 쓰세요

🚀 오늘 바로 실천해볼 한 가지
"회사에서 쓰니까" Java를 쓰는 게 아니라, "생산성이 높으니까" Python을 써보세요.

---

## 한국어 번역 (Korean Translation)

2003년 4월(

이 에세이는 PyCon 2003의 기조 연설에서 발췌한 것입니다.) 100년 후의 삶이 어떤 모습일지 예측하기는 어렵습니다.우리가 확실하게 말할 수 있는 것은 몇 가지뿐입니다.우리는 모두가 하늘을 나는 자동차를 운전하게 될 것이고, 구역법이 완화되어 수백 층 높이의 건물을 허용할 것이며, 대부분의 시간이 어두울 것이며, 여성들은 모두 무술 훈련을 받게 될 것이라는 것을 알고 있습니다.여기서는 이 사진의 한 가지 세부 사항을 확대하고 싶습니다.날아다니는 자동차를 제어하는 ​​소프트웨어를 작성하기 위해 그들은 어떤 종류의 프로그래밍 언어를 사용할 것인가? 이것은 우리가 실제로 이러한 언어를 사용하게 될 것이기 때문에 그다지 생각해 볼 가치가 없습니다. 운이 좋다면 우리는 이 지점에서 저 지점으로 가는 경로에 있는 언어를 사용할 것이기 때문입니다. 제 생각에는 종과 마찬가지로 언어도 진화의 나무를 형성하고 막다른 골목이 사방으로 갈라질 것이라고 생각합니다.우리는 이미 이런 일이 일어나는 것을 볼 수 있습니다.Cobol은 한때 인기를 얻었음에도 불구하고 지적 후손이 없는 것 같습니다.그것은 진화론적 막다른 골목입니다. 네안데르탈인 언어입니다. 저는 자바에서도 비슷한 운명이 있을 것이라고 예측합니다.사람들은 때때로 나에게 "자바가 성공적인 언어가 될 수 없다고 어떻게 단언할 수 있습니까? 자바는 이미 성공적인 언어입니다. "라는 메일을 보내옵니다.그리고 책(특히 책 한 권)이 차지하는 선반 공간이나 취업을 위해 배워야 한다고 믿는 학부생의 수로 성공을 측정한다면 나는 그것이 성공임을 인정합니다.제가 Java가 성공적인 언어가 되지 않을 것이라고 말하는 것은 좀 더 구체적인 의미입니다. Java가 Cobol처럼 진화의 막다른 골목이 될 것이라는 것입니다. 이것은 단지 추측일 뿐입니다.내가 틀렸을 수도 있습니다.여기서 내 요점은 Java를 반대하는 것이 아니라 진화 트리의 문제를 제기하고 사람들이 트리에서 언어 X가 어디에 있는지 묻도록 하는 것입니다.이 질문을 하는 이유는 단지 우리 귀신들이 '100년 뒤에는 내가 그렇게 말했지'라고 말할 수 있도록 하기 위해서가 아닙니다.주요 가지에 가까이 머무르는 것이 지금 프로그래밍하기에 좋은 언어를 찾는 데 유용한 경험적 방법이기 때문입니다. 어느 시점에서든 여러분은 아마도 진화계통의 주요 가지에서 가장 행복할 것입니다.네안데르탈인이 여전히 많았을 때에도 하나가 되었다는 것은 정말 짜증나는 일이었을 것입니다.크로마뇽인들은 끊임없이 와서 당신을 때리고 당신의 음식을 훔쳤을 것입니다. 내가 백년 후에 언어가 어떤 모습일지 알고 싶은 이유는 지금 나무의 어느 가지에 내기를 걸 것인지를 알기 위해서입니다. 가지가 수렴할 수 있기 때문에 언어의 진화는 종의 진화와 다릅니다.예를 들어 Fortran 지부는 Algol의 후손과 합병되는 것 같습니다.이론적으로 이는 종의 경우에도 가능하지만 세포보다 큰 경우에는 발생하지 않을 것입니다. 부분적으로 가능성의 공간이 더 작고 부분적으로 돌연변이가 무작위가 아니기 때문에 수렴이 언어에서 더 가능성이 높습니다.언어 디자이너는 의도적으로 다른 언어의 아이디어를 통합합니다. 프로그래밍 언어의 진화가 어디로 이어질지 생각하는 것은 언어 디자이너에게 특히 유용합니다. 왜냐하면 그에 따라 방향을 잡을 수 있기 때문입니다.그렇다면 "본점에 머무르는 것"은 좋은 언어를 선택하는 방법 이상의 것이 됩니다.이는 언어 설계에 대한 올바른 결정을 내리기 위한 경험적 방법이 됩니다. 모든 프로그래밍 언어는 두 부분으로 나눌 수 있습니다. 공리 역할을 하는 일부 기본 연산자 집합과 원칙적으로 이러한 기본 연산자의 관점에서 작성될 수 있는 나머지 언어입니다. 나는 기본 연산자가 언어의 장기적인 생존에 가장 중요한 요소라고 생각합니다.나머지는 변경할 수 있습니다.집을 살 때 위치를 먼저 고려해야 한다는 법칙과도 같습니다.그 밖의 모든 것은 나중에 고칠 수 있지만 위치는 고칠 수 없습니다. 공리를 잘 선택하는 것뿐만 아니라 공리가 적다는 것이 중요하다고 생각합니다.수학자들은 항상 공리(적을수록 좋음)에 대해 이렇게 느꼈습니다. 제 생각에는 그 공리가 무엇인가에 있다고 생각합니다. 최소한 언어의 핵심을 면밀히 살펴보는 것은 유용한 연습이 되어야 합니다.

e 제거할 수 있는 공리가 있는지 확인합니다.나는 오랜 경력 동안 멍청이가 멍청이를 낳는다는 것을 발견했으며 소프트웨어에서나 침대 밑이나 방 구석에서 이런 일이 일어나는 것을 보았습니다. 나는 진화 계통도의 주요 가지가 가장 작고 깨끗한 핵심을 가진 언어를 통과한다는 예감이 있습니다.자체적으로 작성할 수 있는 언어가 많을수록 좋습니다. 물론 백년 후 프로그래밍 언어가 어떤 모습일지 묻는 것에도 큰 가정을 하고 있습니다.100년 뒤에도 프로그램을 작성할 수 있을까요?우리가 원하는 것을 컴퓨터에게 알려주면 안 될까요? 지금까지 해당 부서에서는 많은 진전이 없었습니다.내 추측으로는 지금으로부터 100년 후에도 사람들은 여전히 ​​우리가 인식할 수 있는 프로그램을 사용하여 컴퓨터에게 무엇을 해야 할지 지시할 것입니다.지금은 프로그램을 작성하여 해결할 수 있는 작업이 있고, 100년 후에는 프로그램을 작성하지 않고도 해결할 수 있는 문제가 있을 수 있습니다. 하지만 오늘날 우리가 하는 유형의 프로그래밍은 여전히 ​​많이 있을 것이라고 생각합니다. 누구나 100년 후에 기술이 어떤 모습일지 예측할 수 있다고 생각하는 것은 주제넘은 것처럼 보일 수 있습니다.그러나 우리 뒤에는 이미 거의 50년의 역사가 있다는 것을 기억하십시오.지난 50년 동안 언어가 얼마나 느리게 진화했는지 생각해 보면 100년을 내다보는 것은 이해하기 쉬운 아이디어입니다. 언어는 실제로 기술이 아니기 때문에 천천히 진화합니다.언어는 표기법입니다.프로그램은 컴퓨터가 해결해주기를 원하는 문제에 대한 공식적인 설명입니다.따라서 프로그래밍 언어의 진화 속도는 교통이나 통신보다는 수학적 표기법의 진화 속도와 더 비슷합니다.수학적 표기법은 진화하지만 기술의 엄청난 도약은 아닙니다. 100년 후에는 어떤 컴퓨터로 만들어지든 지금보다 훨씬 더 빨라질 것이라고 예측하는 것이 안전해 보입니다.무어의 법칙이 계속해서 밝혀지면 74경(73,786,976,294,838,206,464)배 더 빨라질 것입니다.그건 상상하기 힘든 일이죠.그리고 실제로 속도 부문에서 가장 그럴듯한 예측은 무어의 법칙이 작동을 멈추는 것일 수도 있습니다.18개월마다 두 배로 증가한다고 가정되는 모든 것은 결국 일종의 근본적인 한계에 부딪힐 가능성이 높습니다.그러나 나는 컴퓨터가 훨씬 더 빨라질 것이라고 믿는 데 아무런 문제가 없습니다.단지 백만 배 더 빠른 속도로 끝나더라도 이는 프로그래밍 언어의 기본 규칙을 크게 바꿔야 합니다.무엇보다도 현재 느린 언어로 간주되는 언어, 즉 매우 효율적인 코드를 생성하지 않는 언어에 대한 여지가 더 많아질 것입니다. 그러나 일부 응용 프로그램은 여전히 ​​속도를 요구합니다.우리가 컴퓨터로 해결하고 싶은 문제 중 일부는 컴퓨터에 의해 발생합니다.예를 들어, 비디오 이미지를 처리해야 하는 속도는 다른 컴퓨터에서 비디오 이미지를 생성할 수 있는 속도에 따라 달라집니다.그리고 본질적으로 주기를 흡수할 수 있는 무한한 용량을 가진 또 다른 종류의 문제가 있습니다: 이미지 렌더링, 암호화, 시뮬레이션. 일부 응용 프로그램은 점점 더 비효율적이 되고 다른 응용 프로그램은 계속해서 하드웨어가 제공할 수 있는 모든 속도를 요구한다면 컴퓨터가 빨라진다는 것은 언어가 훨씬 더 넓은 범위의 효율성을 처리해야 함을 의미합니다.우리는 이미 이런 일이 일어나는 것을 보았습니다.일부 인기 있는 새로운 언어의 현재 구현은 지난 수십 년의 표준에 비해 놀랄 만큼 낭비입니다. 이는 프로그래밍 언어에서만 발생하는 일이 아닙니다.일반적인 역사적 추세입니다.기술이 발전함에 따라 각 세대는 이전 세대가 낭비라고 여겼던 일을 할 수 있게 되었습니다.30년 전 사람들은 우리가 얼마나 아무렇지도 않게 장거리 전화를 거는지 보고 놀랐을 것입니다.100년 전 사람들은 패키지가 언젠가 보스턴에서 멤피스를 거쳐 뉴욕으로 여행할 것이라는 사실에 훨씬 더 놀랐을 것입니다. 나는 이미 앞으로 100년 동안 더 빠른 하드웨어가 우리에게 제공할 추가 주기에 어떤 일이 일어날지 말씀드릴 수 있습니다.그것들은 거의 모두 낭비될 것입니다.

컴퓨터 전력이 부족할 때 프로그래밍을 배웠습니다.4K TRS-80의 메모리에 맞도록 기본 프로그램에서 모든 공간을 꺼낸 기억이 납니다.이 엄청나게 비효율적인 소프트웨어가 같은 일을 계속 반복하면서 사이클을 소모한다는 생각은 나에게는 다소 역겹게 보입니다.하지만 내 직관은 틀렸다고 생각합니다.나는 가난하게 자랐고, 병원에 가는 것과 같은 중요한 일에도 돈을 쓰는 것을 참을 수 없는 사람과 같습니다. 어떤 종류의 낭비는 정말 역겹습니다.예를 들어, SUV는 결코 고갈되지 않고 오염을 발생시키지 않는 연료를 사용하더라도 틀림없이 역겹습니다.SUV는 심각한 문제에 대한 해결책이기 때문에 역겹습니다.(미니밴을 더욱 남성적으로 보이게 만드는 방법.) 하지만 모든 낭비가 나쁜 것은 아닙니다.이제 이를 지원할 수 있는 인프라가 구축되었으므로 장거리 통화 시간을 계산하는 것이 귀찮아 보이기 시작합니다.자원이 있다면 상대방이 어디에 있든 모든 전화 통화를 하나의 것으로 생각하는 것이 더 우아합니다. 좋은 낭비와 나쁜 낭비가 있습니다.나는 좋은 폐기물, 즉 더 많이 지출함으로써 더 단순한 디자인을 얻을 수 있는 종류에 관심이 있습니다.새롭고 더 빠른 하드웨어에서 발생하는 낭비 사이클의 기회를 어떻게 활용할 수 있습니까? 속도에 대한 욕구는 보잘것없는 컴퓨터와 함께 우리 마음 속에 깊이 뿌리 박혀 있으므로 이를 극복하려면 의식적인 노력이 필요합니다.언어 디자인에서 우리는 편의성의 아주 작은 증가라도 효율성을 교환할 수 있는 상황을 의식적으로 찾아야 합니다. 대부분의 데이터 구조는 속도 때문에 존재합니다.예를 들어, 오늘날 많은 언어에는 문자열과 목록이 모두 있습니다.의미상으로 문자열은 요소가 문자인 목록의 하위 집합입니다.그렇다면 별도의 데이터 유형이 필요한 이유는 무엇입니까?당신은 그렇지 않습니다.문자열은 효율성을 위해서만 존재합니다.그러나 프로그램을 더 빠르게 실행하기 위해 해킹으로 언어의 의미를 어수선하게 만드는 것은 어리석은 일입니다.언어에 문자열이 있다는 것은 조기 최적화의 경우인 것 같습니다. 언어의 핵심을 공리의 집합으로 생각한다면 단순히 효율성을 위해 표현력을 추가하지 않는 추가 공리를 갖는 것은 분명 역겹습니다.효율성은 중요하지만 그것이 그것을 얻는 올바른 방법은 아니라고 생각합니다. 내 생각에 그 문제를 해결하는 올바른 방법은 프로그램의 의미와 구현 세부 사항을 분리하는 것입니다.목록과 문자열을 모두 갖는 대신, 필요한 경우 문자열을 연속 바이트로 배치할 수 있도록 컴파일러 최적화 조언을 제공할 수 있는 방법을 사용하여 목록만 사용하십시오. 대부분의 프로그램에서 속도는 중요하지 않으므로 일반적으로 이러한 종류의 세세한 관리를 귀찮게 할 필요가 없습니다.컴퓨터가 빨라질수록 이는 점점 더 사실이 될 것입니다. 구현에 대해 덜 언급하면 ​​프로그램이 더욱 유연해집니다.프로그램이 작성되는 동안 사양이 변경되는데, 이는 불가피할 뿐만 아니라 바람직할 수도 있습니다. "essay"라는 단어는 "시도하다"를 의미하는 프랑스어 동사 "essayer"에서 유래되었습니다.에세이는 원래 의미에서 무언가를 알아내기 위해 쓰는 것입니다.이것은 소프트웨어에서도 발생합니다.내 생각에 최고의 프로그램 중 일부는 에세이였습니다. 작성자가 자신이 작성하려는 내용을 정확히 언제 시작했는지 몰랐다는 점에서 말입니다. Lisp 해커는 이미 데이터 구조의 유연성이 갖는 가치에 대해 알고 있습니다.우리는 목록을 사용하여 모든 작업을 수행하도록 프로그램의 첫 번째 버전을 작성하는 경향이 있습니다.이러한 초기 버전은 놀라울 정도로 비효율적이어서 자신이 무엇을 하고 있는지 생각하지 않으려는 의식적인 노력이 필요합니다. 마치 스테이크를 먹을 때 그것이 어디서 왔는지 생각하지 않으려는 의식적인 노력이 필요한 것과 같습니다. 백년 후 프로그래머가 찾고 있는 것은 무엇보다도 최소한의 노력으로 믿을 수 없을 정도로 비효율적인 프로그램 버전 1을 함께 만들 수 있는 언어입니다.적어도 그것이 오늘날의 용어로 설명하는 방법입니다.그들이 말할 것은 프로그래밍하기 쉬운 언어를 원한다는 것입니다. 비효율적인 소프트웨어는 역겹지 않습니다.무엇'

총체적인 언어는 프로그래머가 불필요한 작업을 하게 만드는 언어입니다.프로그래머 시간을 낭비하는 것은 기계 시간을 낭비하는 것이 아니라 진정한 비효율성입니다.이는 컴퓨터가 빨라질수록 더욱 분명해질 것입니다. 문자열을 없애는 것은 이미 우리가 생각해 볼 수 있는 일이라고 생각합니다.우리는 Arc에서 해냈는데, 성공한 것 같습니다.정규식으로 설명하기 어려운 일부 작업은 재귀 함수로 쉽게 설명할 수 있습니다. 데이터 구조의 평면화는 얼마나 진행될까요?양심적으로 넓어진 마음으로 나조차도 충격을 받을 가능성을 떠올릴 수 있다.예를 들어 배열을 제거할까요?결국, 그들은 키가 정수 벡터인 해시 테이블의 하위 집합일 뿐입니다.해시 테이블 자체를 리스트로 대체할 것인가? 그보다 더 충격적인 전망이 있다.예를 들어 McCarthy가 1960년에 설명한 Lisp에는 숫자가 없었습니다.논리적으로 숫자를 목록으로 표현할 수 있기 때문에 숫자에 대한 별도의 개념이 필요하지 않습니다. 정수 n은 n 요소의 목록으로 표현될 수 있습니다.이런 식으로 수학을 할 수 있습니다.참을 수 없을 정도로 비효율적입니다. 실제로 실제로 숫자를 목록으로 구현하는 것을 제안한 사람은 아무도 없습니다.사실, McCarthy의 1960년 논문은 당시에는 전혀 실행될 의도가 아니었습니다.이는 튜링 머신에 대한 보다 우아한 대안을 만들려는 시도인 이론적 연습이었습니다.누군가가 예기치 않게 이 문서를 가져와 작동하는 Lisp 해석기로 번역했을 때 숫자는 확실히 목록으로 표시되지 않았습니다.다른 모든 언어와 마찬가지로 이진수로 표현되었습니다. 프로그래밍 언어가 기본 데이터 유형인 숫자를 제거할 수 있을까요?나는 이것을 진지한 질문이라기보다는 미래를 가지고 치킨게임을 하는 방법으로서 묻는다.이는 저항할 수 없는 힘이 움직일 수 없는 물체를 만나는 가상의 사례와 같습니다. 여기서는 상상할 수 없을 정도로 큰 자원을 만나는 상상할 수 없을 정도로 비효율적인 구현입니다.왜 안되는지 모르겠습니다.미래는 꽤 길다.핵심 언어에서 공리의 수를 줄이기 위해 우리가 할 수 있는 일이 있다면, 그것은 t가 무한대에 접근할 때 베팅할 측면인 것 같습니다.백년이 지나도 아이디어가 여전히 견딜 수 없을 것 같다면 아마도 천년이 지나도 그렇지 않을 것입니다. 이 점을 분명히 하기 위해 모든 수치 계산이 실제로 목록을 사용하여 수행될 것이라고 제안하는 것은 아닙니다.나는 구현에 대한 추가 표기에 앞서 핵심 언어를 이런 방식으로 정의할 것을 제안합니다.실제로 수학을 원하는 모든 프로그램은 숫자를 이진수로 나타낼 수 있지만 이는 핵심 언어 의미 체계의 일부가 아닌 최적화일 것입니다. 주기를 소모하는 또 다른 방법은 응용 프로그램과 하드웨어 사이에 많은 소프트웨어 계층을 두는 것입니다.이것 역시 이미 일어나고 있는 추세입니다. 최근의 많은 언어가 바이트 코드로 컴파일됩니다.Bill Woods는 경험상 각 해석 단계의 속도가 10배 정도 필요하다고 나에게 말한 적이 있습니다.이 추가 비용으로 유연성을 얻을 수 있습니다. Arc의 첫 번째 버전은 이러한 종류의 다단계 속도 저하와 이에 상응하는 이점을 지닌 극단적인 사례였습니다.이는 McCarthy의 원본 Lisp 논문에 정의된 eval 함수와 확실히 유사하며 Common Lisp 위에 작성된 고전적인 "메타원형" 해석기였습니다.전체 내용이 고작 몇백 줄의 코드였기 때문에 이해하고 변경하기가 매우 쉬웠습니다.우리가 사용한 Common Lisp인 CLisp 자체는 바이트 코드 인터프리터 위에서 실행됩니다.그래서 여기에는 두 가지 수준의 해석이 있었는데 그 중 하나(가장 높은 수준)는 놀라울 정도로 비효율적이었고 언어는 사용할 수 있었습니다.거의 사용할 수 없음을 인정하지만 사용할 수 있습니다. 소프트웨어를 여러 계층으로 작성하는 것은 응용 프로그램 내에서도 강력한 기술입니다.상향식 프로그래밍은 프로그램을 일련의 계층으로 작성하는 것을 의미하며, 각 계층은 위 계층의 언어 역할을 합니다.이 접근 방식은 더 작고 유연한 프로그램을 생성하는 경향이 있습니다.이는 또한 성배, 재사용성을 향한 최선의 길이기도 합니다.언어는 정의상 재사용이 가능합니다.당신의 더 많은

해당 유형의 응용 프로그램을 작성하기 위해 언어로 응용 프로그램을 푸시할 수 있으면 더 많은 소프트웨어를 재사용할 수 있습니다. 어쨌든 재사용 가능성이라는 개념은 1980년대 객체 지향 프로그래밍에 결합되었으며 반대되는 증거는 아무리 많아도 이를 자유롭게 할 수 없는 것 같습니다.그러나 일부 객체 지향 소프트웨어는 재사용이 가능하지만 이를 재사용 가능하게 만드는 것은 객체 지향이 아니라 상향식이기 때문입니다.라이브러리를 생각해 보세요. 객체 지향 스타일로 작성되었는지 여부에 관계없이 언어이기 때문에 재사용이 가능합니다. 그런데 객체 지향 프로그래밍의 종말을 예측하지는 않습니다.특정 전문 영역을 제외하고는 좋은 프로그래머에게 많은 것을 제공한다고 생각하지 않지만 대규모 조직에서는 거부할 수 없습니다.객체 지향 프로그래밍은 스파게티 코드를 작성하는 지속 가능한 방법을 제공합니다.이를 통해 프로그램을 일련의 패치로 추가할 수 있습니다.대규모 조직은 항상 이런 방식으로 소프트웨어를 개발하는 경향이 있으며, 이것이 오늘날처럼 100년 후에도 사실이 될 것으로 기대합니다.우리가 미래에 대해 이야기하는 한 병렬 계산에 대해 이야기하는 것이 더 나을 것입니다. 왜냐하면 병렬 계산이 바로 이 아이디어가 살아 있는 곳이기 때문입니다.즉, 언제 이야기하든 병렬 계산은 미래에 일어날 일인 것 같습니다. 미래가 그것을 따라잡을 수 있을까요?사람들은 병렬 계산에 대해 적어도 20년 동안 임박한 일로 이야기해 왔지만 지금까지는 프로그래밍 실무에 큰 영향을 미치지 않았습니다.아니면 그렇지 않습니까?이미 칩 설계자들은 그것에 대해 생각해야 하고 다중 CPU 컴퓨터에서 시스템 소프트웨어를 작성하려는 사람들도 그렇게 생각해야 합니다. 실제 질문은 병렬 처리가 추상화의 사다리에서 얼마나 멀리 올라갈 것인가입니다.100년 후에는 애플리케이션 프로그래머에게도 영향을 미치게 될까요?아니면 컴파일러 작성자가 생각하지만 일반적으로 응용 프로그램의 소스 코드에서는 보이지 않는 것입니까? 한 가지 가능성 있는 것은 병렬 처리에 대한 대부분의 기회가 낭비된다는 것입니다.이것은 우리에게 주어진 추가 컴퓨터 성능의 대부분이 낭비될 것이라는 나의 보다 일반적인 예측의 특별한 경우입니다.기본 하드웨어의 엄청난 속도와 마찬가지로 병렬 처리도 명시적으로 요청하면 사용할 수 있지만 일반적으로 사용되지는 않을 것으로 예상됩니다.이는 우리가 100년 동안 갖고 있는 종류의 병렬 처리가 특수한 응용을 제외하고는 대규모 병렬 처리가 되지 않을 것임을 의미합니다.나는 일반 프로그래머의 경우 모든 것이 병렬로 실행되는 프로세스를 분기할 수 있는 것과 비슷할 것으로 기대합니다. 그리고 이는 데이터 구조의 특정 구현을 요청하는 것과 마찬가지로 프로그램 수명의 후반부에 최적화하려고 할 때 수행하는 작업이 될 것입니다.버전 1은 특정 데이터 표현에서 얻을 수 있는 이점을 무시하는 것처럼 일반적으로 병렬 계산에서 얻을 수 있는 모든 이점을 무시합니다. 특별한 종류의 응용 프로그램을 제외하고 병렬 처리는 100년 동안 작성된 프로그램에 널리 퍼지지 않습니다.만약 그렇다면 성급한 최적화가 될 것입니다. 100년 후에는 프로그래밍 언어가 몇 개나 될까요?최근에는 새로운 프로그래밍 언어가 엄청나게 많이 나오는 것 같습니다.그 이유 중 하나는 더 빠른 하드웨어로 인해 프로그래머가 애플리케이션에 따라 속도와 편의성 사이에서 서로 다른 절충안을 만들 수 있다는 것입니다.이것이 실제 추세라면, 100년 후에 우리가 갖게 될 하드웨어는 그 정도만 늘려야 합니다. 그러나 100년 안에 널리 사용되는 언어는 소수에 불과할 수도 있습니다.제가 이렇게 말하는 이유 중 하나는 낙관주의입니다. 정말 잘했다면 느린 버전 1을 작성하는 데 이상적인 언어를 만들 수 있을 뿐만 아니라 컴파일러에 대한 올바른 최적화 조언을 통해 필요할 때 매우 빠른 코드도 생성할 수 있을 것 같습니다.따라서 저는 낙관적이므로 수용 가능한 효율성과 최대 효율성 사이에 큰 격차가 있음에도 불구하고 100년 후에 프로그래머는 대부분의 효율성을 포괄할 수 있는 언어를 갖게 될 것이라고 예측하겠습니다. 이 격차가 넓어짐에 따라 프로파일러는 점점 더 많아질 것입니다.

중요해요.지금은 프로파일링에 거의 관심을 기울이지 않습니다.많은 사람들은 여전히 ​​빠른 애플리케이션을 얻는 방법은 빠른 코드를 생성하는 컴파일러를 작성하는 것이라고 믿는 것 같습니다.허용 가능한 성능과 최대 성능 사이의 격차가 커지면서 빠른 응용 프로그램을 얻는 방법은 하나에서 다른 것으로 좋은 가이드를 갖는 것이라는 것이 점점 더 분명해질 것입니다. 언어가 몇 개만 있을 수 있다고 말할 때 도메인별 "작은 언어"는 포함되지 않습니다.이런 임베디드 언어는 좋은 아이디어라고 생각하고, 확산되길 기대합니다.하지만 나는 사용자들이 그 밑에 있는 범용 언어를 볼 수 있을 만큼 충분히 얇은 스킨으로 작성되기를 기대합니다. 미래의 언어는 누가 디자인할까요?지난 10년 동안 가장 흥미로운 추세 중 하나는 Perl, Python, Ruby와 같은 오픈 소스 언어의 등장이었습니다.언어 설계가 해커들에 의해 장악되고 있습니다.지금까지의 결과는 지저분하지만 고무적입니다.예를 들어, Perl에는 놀랍도록 참신한 아이디어가 있습니다.많은 경우가 놀라울 정도로 나쁩니다. 하지만 야심 찬 노력은 언제나 그렇습니다.현재의 돌연변이 속도로 볼 때, 하나님께서는 Perl이 100년 후에 어떻게 진화할지 아십니다. 할 수 없는 사람이 가르친다는 것은 사실이 아니지만(내가 아는 최고의 해커 중 일부는 교수입니다) 가르치는 사람이 할 수 없는 일이 많다는 것은 사실입니다.연구에서는 카스트 제한을 강제하고 있습니다.모든 학문 분야에는 연구해도 괜찮은 주제가 있고 그렇지 않은 주제도 있습니다.불행하게도 허용되는 주제와 금지되는 주제의 구별은 일반적으로 좋은 결과를 얻는 데 얼마나 중요한지보다는 연구 논문에 설명될 때 작품이 얼마나 지적으로 들리는지에 따라 결정됩니다.극단적인 경우는 아마도 문학일 것이다.문학을 공부하는 사람들은 그것을 생산하는 사람들에게 조금이라도 도움이 될 만한 말을 거의 하지 않습니다. 비록 과학 분야의 상황이 더 나을지라도, 당신이 할 수 있는 일과 좋은 언어를 만들어내는 일 사이의 중복은 괴로울 정도로 작습니다.(Olin Shivers는 이에 대해 유창하게 불평했습니다.) 예를 들어, 정적 타이핑이 진정한 매크로를 배제하는 것처럼 보임에도 불구하고 유형은 연구 논문의 무궁무진한 소스인 것 같습니다. 제 생각에는 유형이 없으면 어떤 언어도 사용할 가치가 없습니다. 추세는 단순히 "연구"가 아닌 오픈 소스 프로젝트로 언어를 개발하는 것이 아니라 컴파일러 작성자가 아닌 이를 사용해야 하는 응용 프로그램 프로그래머가 언어를 설계하는 방향입니다.이는 좋은 추세로 보이며 계속 유지될 것으로 예상됩니다.예측이 거의 불가능한 백년 후의 물리학과 달리, 백년 후 사용자에게 어필할 언어를 지금 설계하는 것이 원칙적으로 가능할 수도 있다고 생각합니다. 언어를 설계하는 한 가지 방법은 언어를 번역할 수 있는 컴파일러가 있든, 실행할 수 있는 하드웨어가 있든 상관없이 작성하고 싶은 프로그램을 그냥 적어 두는 것입니다.이렇게 하면 무제한의 자원을 확보할 수 있습니다.100년 후에도 오늘날에도 무한한 자원을 상상할 수 있어야 할 것 같습니다. 어떤 프로그램을 작성하고 싶습니까?무엇이든 최소한의 일입니다.완전히는 아니지만 프로그래밍에 대한 아이디어가 현재 익숙한 언어의 영향을 받지 않았다면 무엇이든 가장 효과가 없을 것입니다.그러한 영향력은 너무나 널리 퍼져 있기 때문에 이를 극복하려면 많은 노력이 필요합니다.우리처럼 게으른 생물에게는 최소한의 노력으로 프로그램을 표현하는 방법이 명백하다고 생각할 것입니다.사실 무엇이 가능한지에 대한 우리의 생각은 우리가 생각하는 언어에 따라 제한되는 경향이 있어서 프로그램을 더 쉽게 공식화한다는 것은 매우 놀라운 일처럼 보입니다.그것들은 자연스럽게 빠져들게 되는 것이 아니라 당신이 발견해야 하는 것입니다. 여기서 유용한 비결 중 하나는 프로그램의 길이를 작성하는 데 드는 작업량에 대한 근사치로 사용하는 것입니다.물론 문자의 길이가 아니라 개별 구문 요소의 길이, 즉 기본적으로 구문 분석 트리의 크기입니다.가장 짧은 프로그램이 글쓰기에 가장 적은 작업이라는 말은 사실이 아닐 수도 있습니다.

하지만 충분히 가까워서 작업이 가장 적은 근처의 모호하고 간결한 목표를 목표로 삼는 것이 더 좋습니다.그런 다음 언어 설계를 위한 알고리즘은 다음과 같습니다. 프로그램을 보고 이를 더 짧게 작성할 수 있는 방법이 있습니까? 실제로 가상의 100년 언어로 프로그램을 작성하는 것은 핵심에 얼마나 가까운지에 따라 다양한 정도로 작동합니다.지금 작성할 수 있는 루틴을 정렬하세요.하지만 지금은 100년 후에 어떤 종류의 도서관이 필요할지 예측하기 어려울 것입니다.아마도 아직 존재하지도 않는 도메인을 위한 라이브러리가 많을 것입니다.예를 들어 SETI@home이 작동한다면 외계인과 통신하기 위한 라이브러리가 필요합니다.물론 이미 XML로 통신할 만큼 충분히 발전하지 않은 이상 극단적인 경우에는 오늘날 핵심 언어를 디자인할 수 있을 것이라고 생각합니다.사실 어떤 사람들은 이미 대부분이 1958년에 설계되었다고 주장할 수도 있습니다. 만약 백년 언어가 오늘날 사용 가능하다면 우리는 그 언어로 프로그래밍하고 싶을까요?이 질문에 대답하는 한 가지 방법은 되돌아보는 것입니다.1960년에 현재의 프로그래밍 언어가 있었다면 그것을 사용하고 싶은 사람이 있었을까요? 어떤 면에서 대답은 '아니오'입니다.오늘날의 언어는 1960년에는 존재하지 않았던 인프라를 가정합니다. 예를 들어 Python과 같이 들여쓰기가 중요한 언어는 프린터 터미널에서 잘 작동하지 않습니다.하지만 그러한 문제를 제쳐두고, 예를 들어 프로그램이 모두 종이에 작성되었다고 가정하면, 1960년대 프로그래머들이 지금 우리가 사용하는 언어로 프로그램을 작성하는 것을 좋아했을까요? 저는 그렇다고 생각합니다.프로그램이 무엇인지에 대한 생각에 초기 언어의 인공물을 내장한 상상력이 부족한 사람들 중 일부는 어려움을 겪었을 수도 있습니다.(포인터 연산을 하지 않고 어떻게 데이터를 조작할 수 있습니까? gotos 없이 어떻게 순서도를 구현할 수 있습니까?) 하지만 가장 똑똑한 프로그래머라면 현대 언어를 가지고 있었다면 이를 최대한 활용하는 데 어려움이 없었을 것이라고 생각합니다. 지금 우리가 100년 된 언어를 가지고 있었다면 적어도 훌륭한 의사 코드를 만들 수 있었을 것입니다.소프트웨어를 작성하는 데 사용하는 것은 어떻습니까?100년 언어는 일부 응용 프로그램에 대해 빠른 코드를 생성해야 하므로 아마도 하드웨어에서 허용 가능한 수준으로 잘 실행될 만큼 효율적인 코드를 생성할 수 있을 것입니다.우리는 100년 후에는 사용자보다 더 많은 최적화 조언을 주어야 할 수도 있지만 여전히 순익이 될 수 있습니다. 이제 우리는 두 가지 아이디어를 결합하면 흥미로운 가능성을 제안할 수 있습니다. (1) 100년 언어는 원칙적으로 오늘날 설계될 수 있으며 (2) 그러한 언어가 존재한다면 오늘날 프로그래밍하기에 좋을 수 있습니다.이런 생각들이 이렇게 정리되어 있는 것을 보면 생각이 안 날 정도인데, 지금이라도 백년 언어를 써보면 어떨까? 언어 디자인 작업을 할 때, 그런 목표를 갖고 의식적으로 염두에 두는 게 좋은 것 같아요.운전을 배울 때 그들이 가르치는 원리 중 하나는 후드를 도로에 그려진 줄무늬에 맞춰 정렬하는 것이 아니라 멀리 있는 특정 지점을 겨냥하여 자동차를 정렬하는 것입니다.당신이 관심을 갖는 것이 앞으로 10피트 안에 무슨 일이 일어나는가 하는 것뿐이라고 해도 이것이 정답입니다.나는 우리가 프로그래밍 언어에서도 같은 일을 할 수 있고 해야 한다고 생각합니다.Notes저는 Lisp Machine Lisp가 선언(동적 변수 제외)이 단지 최적화 조언일 뿐 올바른 프로그램의 의미를 바꾸지 않는다는 원칙을 구현한 최초의 언어라고 믿습니다.Common Lisp가 이것을 명시적으로 언급한 최초의 사람인 것 같습니다. 이 초안을 읽어준 Trevor Blackwell, Robert Morris 및 Dan Giffin에게 감사하고, PyCon에서 연설하도록 초대해 준 Guido van Rossum, Jeremy Hylton 및 나머지 Python 팀원들에게 감사드립니다.Hackers & Painters에서 이 에세이와 다른 14개의 에세이를 찾을 수 있습니다.

---

## 원문 (Original Essay)

April 2003(This essay is derived from a keynote talk at PyCon 2003.)It's hard to predict what life will be like in a hundred years. There are only a few things we can say with certainty. We know that everyone will drive flying cars, that zoning laws will be relaxed to allow buildings hundreds of stories tall, that it will be dark most of the time, and that women will all be trained in the martial arts. Here I want to zoom in on one detail of this picture. What kind of programming language will they use to write the software controlling those flying cars?This is worth thinking about not so much because we'll actually get to use these languages as because, if we're lucky, we'll use languages on the path from this point to that.I think that, like species, languages will form evolutionary trees, with dead-ends branching off all over. We can see this happening already. Cobol, for all its sometime popularity, does not seem to have any intellectual descendants. It is an evolutionary dead-end-- a Neanderthal language.I predict a similar fate for Java. People sometimes send me mail saying, "How can you say that Java won't turn out to be a successful language? It's already a successful language." And I admit that it is, if you measure success by shelf space taken up by books on it (particularly individual books on it), or by the number of undergrads who believe they have to learn it to get a job. When I say Java won't turn out to be a successful language, I mean something more specific: that Java will turn out to be an evolutionary dead-end, like Cobol.This is just a guess. I may be wrong. My point here is not to dis Java, but to raise the issue of evolutionary trees and get people asking, where on the tree is language X? The reason to ask this question isn't just so that our ghosts can say, in a hundred years, I told you so. It's because staying close to the main branches is a useful heuristic for finding languages that will be good to program in now.At any given time, you're probably happiest on the main branches of an evolutionary tree. Even when there were still plenty of Neanderthals, it must have sucked to be one. The Cro-Magnons would have been constantly coming over and beating you up and stealing your food.The reason I want to know what languages will be like in a hundred years is so that I know what branch of the tree to bet on now.The evolution of languages differs from the evolution of species because branches can converge. The Fortran branch, for example, seems to be merging with the descendants of Algol. In theory this is possible for species too, but it's not likely to have happened to any bigger than a cell.Convergence is more likely for languages partly because the space of possibilities is smaller, and partly because mutations are not random. Language designers deliberately incorporate ideas from other languages.It's especially useful for language designers to think about where the evolution of programming languages is likely to lead, because they can steer accordingly. In that case, "stay on a main branch" becomes more than a way to choose a good language. It becomes a heuristic for making the right decisions about language design.Any programming language can be divided into two parts: some set of fundamental operators that play the role of axioms, and the rest of the language, which could in principle be written in terms of these fundamental operators.I think the fundamental operators are the most important factor in a language's long term survival. The rest you can change. It's like the rule that in buying a house you should consider location first of all. Everything else you can fix later, but you can't fix the location.I think it's important not just that the axioms be well chosen, but that there be few of them. Mathematicians have always felt this way about axioms-- the fewer, the better-- and I think they're onto something.At the very least, it has to be a useful exercise to look closely at the core of a language to see if there are any axioms that could be weeded out. I've found in my long career as a slob that cruft breeds cruft, and I've seen this happen in software as well as under beds and in the corners of rooms.I have a hunch that the main branches of the evolutionary tree pass through the languages that have the smallest, cleanest cores. The more of a language you can write in itself, the better.Of course, I'm making a big assumption in even asking what programming languages will be like in a hundred years. Will we even be writing programs in a hundred years? Won't we just tell computers what we want them to do?There hasn't been a lot of progress in that department so far. My guess is that a hundred years from now people will still tell computers what to do using programs we would recognize as such. There may be tasks that we solve now by writing programs and which in a hundred years you won't have to write programs to solve, but I think there will still be a good deal of programming of the type that we do today.It may seem presumptuous to think anyone can predict what any technology will look like in a hundred years. But remember that we already have almost fifty years of history behind us. Looking forward a hundred years is a graspable idea when we consider how slowly languages have evolved in the past fifty.Languages evolve slowly because they're not really technologies. Languages are notation. A program is a formal description of the problem you want a computer to solve for you. So the rate of evolution in programming languages is more like the rate of evolution in mathematical notation than, say, transportation or communications. Mathematical notation does evolve, but not with the giant leaps you see in technology.Whatever computers are made of in a hundred years, it seems safe to predict they will be much faster than they are now. If Moore's Law continues to put out, they will be 74 quintillion (73,786,976,294,838,206,464) times faster. That's kind of hard to imagine. And indeed, the most likely prediction in the speed department may be that Moore's Law will stop working. Anything that is supposed to double every eighteen months seems likely to run up against some kind of fundamental limit eventually. But I have no trouble believing that computers will be very much faster. Even if they only end up being a paltry million times faster, that should change the ground rules for programming languages substantially. Among other things, there will be more room for what would now be considered slow languages, meaning languages that don't yield very efficient code.And yet some applications will still demand speed. Some of the problems we want to solve with computers are created by computers; for example, the rate at which you have to process video images depends on the rate at which another computer can generate them. And there is another class of problems which inherently have an unlimited capacity to soak up cycles: image rendering, cryptography, simulations.If some applications can be increasingly inefficient while others continue to demand all the speed the hardware can deliver, faster computers will mean that languages have to cover an ever wider range of efficiencies. We've seen this happening already. Current implementations of some popular new languages are shockingly wasteful by the standards of previous decades.This isn't just something that happens with programming languages. It's a general historical trend. As technologies improve, each generation can do things that the previous generation would have considered wasteful. People thirty years ago would be astonished at how casually we make long distance phone calls. People a hundred years ago would be even more astonished that a package would one day travel from Boston to New York via Memphis.I can already tell you what's going to happen to all those extra cycles that faster hardware is going to give us in the next hundred years. They're nearly all going to be wasted.I learned to program when computer power was scarce. I can remember taking all the spaces out of my Basic programs so they would fit into the memory of a 4K TRS-80. The thought of all this stupendously inefficient software burning up cycles doing the same thing over and over seems kind of gross to me. But I think my intuitions here are wrong. I'm like someone who grew up poor, and can't bear to spend money even for something important, like going to the doctor.Some kinds of waste really are disgusting. SUVs, for example, would arguably be gross even if they ran on a fuel which would never run out and generated no pollution. SUVs are gross because they're the solution to a gross problem. (How to make minivans look more masculine.) But not all waste is bad. Now that we have the infrastructure to support it, counting the minutes of your long-distance calls starts to seem niggling. If you have the resources, it's more elegant to think of all phone calls as one kind of thing, no matter where the other person is.There's good waste, and bad waste. I'm interested in good waste-- the kind where, by spending more, we can get simpler designs. How will we take advantage of the opportunities to waste cycles that we'll get from new, faster hardware?The desire for speed is so deeply engrained in us, with our puny computers, that it will take a conscious effort to overcome it. In language design, we should be consciously seeking out situations where we can trade efficiency for even the smallest increase in convenience.Most data structures exist because of speed. For example, many languages today have both strings and lists. Semantically, strings are more or less a subset of lists in which the elements are characters. So why do you need a separate data type? You don't, really. Strings only exist for efficiency. But it's lame to clutter up the semantics of the language with hacks to make programs run faster. Having strings in a language seems to be a case of premature optimization.If we think of the core of a language as a set of axioms, surely it's gross to have additional axioms that add no expressive power, simply for the sake of efficiency. Efficiency is important, but I don't think that's the right way to get it.The right way to solve that problem, I think, is to separate the meaning of a program from the implementation details. Instead of having both lists and strings, have just lists, with some way to give the compiler optimization advice that will allow it to lay out strings as contiguous bytes if necessary.Since speed doesn't matter in most of a program, you won't ordinarily need to bother with this sort of micromanagement. This will be more and more true as computers get faster.Saying less about implementation should also make programs more flexible. Specifications change while a program is being written, and this is not only inevitable, but desirable.The word "essay" comes from the French verb "essayer", which means "to try". An essay, in the original sense, is something you write to try to figure something out. This happens in software too. I think some of the best programs were essays, in the sense that the authors didn't know when they started exactly what they were trying to write.Lisp hackers already know about the value of being flexible with data structures. We tend to write the first version of a program so that it does everything with lists. These initial versions can be so shockingly inefficient that it takes a conscious effort not to think about what they're doing, just as, for me at least, eating a steak requires a conscious effort not to think where it came from.What programmers in a hundred years will be looking for, most of all, is a language where you can throw together an unbelievably inefficient version 1 of a program with the least possible effort. At least, that's how we'd describe it in present-day terms. What they'll say is that they want a language that's easy to program in.Inefficient software isn't gross. What's gross is a language that makes programmers do needless work. Wasting programmer time is the true inefficiency, not wasting machine time. This will become ever more clear as computers get faster.I think getting rid of strings is already something we could bear to think about. We did it in Arc, and it seems to be a win; some operations that would be awkward to describe as regular expressions can be described easily as recursive functions.How far will this flattening of data structures go? I can think of possibilities that shock even me, with my conscientiously broadened mind. Will we get rid of arrays, for example? After all, they're just a subset of hash tables where the keys are vectors of integers. Will we replace hash tables themselves with lists?There are more shocking prospects even than that. The Lisp that McCarthy described in 1960, for example, didn't have numbers. Logically, you don't need to have a separate notion of numbers, because you can represent them as lists: the integer n could be represented as a list of n elements. You can do math this way. It's just unbearably inefficient.No one actually proposed implementing numbers as lists in practice. In fact, McCarthy's 1960 paper was not, at the time, intended to be implemented at all. It was a theoretical exercise, an attempt to create a more elegant alternative to the Turing Machine. When someone did, unexpectedly, take this paper and translate it into a working Lisp interpreter, numbers certainly weren't represented as lists; they were represented in binary, as in every other language.Could a programming language go so far as to get rid of numbers as a fundamental data type? I ask this not so much as a serious question as as a way to play chicken with the future. It's like the hypothetical case of an irresistible force meeting an immovable object-- here, an unimaginably inefficient implementation meeting unimaginably great resources. I don't see why not. The future is pretty long. If there's something we can do to decrease the number of axioms in the core language, that would seem to be the side to bet on as t approaches infinity. If the idea still seems unbearable in a hundred years, maybe it won't in a thousand.Just to be clear about this, I'm not proposing that all numerical calculations would actually be carried out using lists. I'm proposing that the core language, prior to any additional notations about implementation, be defined this way. In practice any program that wanted to do any amount of math would probably represent numbers in binary, but this would be an optimization, not part of the core language semantics.Another way to burn up cycles is to have many layers of software between the application and the hardware. This too is a trend we see happening already: many recent languages are compiled into byte code. Bill Woods once told me that, as a rule of thumb, each layer of interpretation costs a factor of 10 in speed. This extra cost buys you flexibility.The very first version of Arc was an extreme case of this sort of multi-level slowness, with corresponding benefits. It was a classic "metacircular" interpreter written on top of Common Lisp, with a definite family resemblance to the eval function defined in McCarthy's original Lisp paper. The whole thing was only a couple hundred lines of code, so it was very easy to understand and change. The Common Lisp we used, CLisp, itself runs on top of a byte code interpreter. So here we had two levels of interpretation, one of them (the top one) shockingly inefficient, and the language was usable. Barely usable, I admit, but usable.Writing software as multiple layers is a powerful technique even within applications. Bottom-up programming means writing a program as a series of layers, each of which serves as a language for the one above. This approach tends to yield smaller, more flexible programs. It's also the best route to that holy grail, reusability. A language is by definition reusable. The more of your application you can push down into a language for writing that type of application, the more of your software will be reusable.Somehow the idea of reusability got attached to object-oriented programming in the 1980s, and no amount of evidence to the contrary seems to be able to shake it free. But although some object-oriented software is reusable, what makes it reusable is its bottom-upness, not its object-orientedness. Consider libraries: they're reusable because they're language, whether they're written in an object-oriented style or not.I don't predict the demise of object-oriented programming, by the way. Though I don't think it has much to offer good programmers, except in certain specialized domains, it is irresistible to large organizations. Object-oriented programming offers a sustainable way to write spaghetti code. It lets you accrete programs as a series of patches. Large organizations always tend to develop software this way, and I expect this to be as true in a hundred years as it is today. As long as we're talking about the future, we had better talk about parallel computation, because that's where this idea seems to live. That is, no matter when you're talking, parallel computation seems to be something that is going to happen in the future.Will the future ever catch up with it? People have been talking about parallel computation as something imminent for at least 20 years, and it hasn't affected programming practice much so far. Or hasn't it? Already chip designers have to think about it, and so must people trying to write systems software on multi-cpu computers.The real question is, how far up the ladder of abstraction will parallelism go? In a hundred years will it affect even application programmers? Or will it be something that compiler writers think about, but which is usually invisible in the source code of applications?One thing that does seem likely is that most opportunities for parallelism will be wasted. This is a special case of my more general prediction that most of the extra computer power we're given will go to waste. I expect that, as with the stupendous speed of the underlying hardware, parallelism will be something that is available if you ask for it explicitly, but ordinarily not used. This implies that the kind of parallelism we have in a hundred years will not, except in special applications, be massive parallelism. I expect for ordinary programmers it will be more like being able to fork off processes that all end up running in parallel.And this will, like asking for specific implementations of data structures, be something that you do fairly late in the life of a program, when you try to optimize it. Version 1s will ordinarily ignore any advantages to be got from parallel computation, just as they will ignore advantages to be got from specific representations of data.Except in special kinds of applications, parallelism won't pervade the programs that are written in a hundred years. It would be premature optimization if it did.How many programming languages will there be in a hundred years? There seem to be a huge number of new programming languages lately. Part of the reason is that faster hardware has allowed programmers to make different tradeoffs between speed and convenience, depending on the application. If this is a real trend, the hardware we'll have in a hundred years should only increase it.And yet there may be only a few widely-used languages in a hundred years. Part of the reason I say this is optimism: it seems that, if you did a really good job, you could make a language that was ideal for writing a slow version 1, and yet with the right optimization advice to the compiler, would also yield very fast code when necessary. So, since I'm optimistic, I'm going to predict that despite the huge gap they'll have between acceptable and maximal efficiency, programmers in a hundred years will have languages that can span most of it.As this gap widens, profilers will become increasingly important. Little attention is paid to profiling now. Many people still seem to believe that the way to get fast applications is to write compilers that generate fast code. As the gap between acceptable and maximal performance widens, it will become increasingly clear that the way to get fast applications is to have a good guide from one to the other.When I say there may only be a few languages, I'm not including domain-specific "little languages". I think such embedded languages are a great idea, and I expect them to proliferate. But I expect them to be written as thin enough skins that users can see the general-purpose language underneath.Who will design the languages of the future? One of the most exciting trends in the last ten years has been the rise of open-source languages like Perl, Python, and Ruby. Language design is being taken over by hackers. The results so far are messy, but encouraging. There are some stunningly novel ideas in Perl, for example. Many are stunningly bad, but that's always true of ambitious efforts. At its current rate of mutation, God knows what Perl might evolve into in a hundred years.It's not true that those who can't do, teach (some of the best hackers I know are professors), but it is true that there are a lot of things that those who teach can't do. Research imposes constraining caste restrictions. In any academic field there are topics that are ok to work on and others that aren't. Unfortunately the distinction between acceptable and forbidden topics is usually based on how intellectual the work sounds when described in research papers, rather than how important it is for getting good results. The extreme case is probably literature; people studying literature rarely say anything that would be of the slightest use to those producing it.Though the situation is better in the sciences, the overlap between the kind of work you're allowed to do and the kind of work that yields good languages is distressingly small. (Olin Shivers has grumbled eloquently about this.) For example, types seem to be an inexhaustible source of research papers, despite the fact that static typing seems to preclude true macros-- without which, in my opinion, no language is worth using.The trend is not merely toward languages being developed as open-source projects rather than "research", but toward languages being designed by the application programmers who need to use them, rather than by compiler writers. This seems a good trend and I expect it to continue. Unlike physics in a hundred years, which is almost necessarily impossible to predict, I think it may be possible in principle to design a language now that would appeal to users in a hundred years.One way to design a language is to just write down the program you'd like to be able to write, regardless of whether there is a compiler that can translate it or hardware that can run it. When you do this you can assume unlimited resources. It seems like we ought to be able to imagine unlimited resources as well today as in a hundred years.What program would one like to write? Whatever is least work. Except not quite: whatever would be least work if your ideas about programming weren't already influenced by the languages you're currently used to. Such influence can be so pervasive that it takes a great effort to overcome it. You'd think it would be obvious to creatures as lazy as us how to express a program with the least effort. In fact, our ideas about what's possible tend to be so limited by whatever language we think in that easier formulations of programs seem very surprising. They're something you have to discover, not something you naturally sink into.One helpful trick here is to use the length of the program as an approximation for how much work it is to write. Not the length in characters, of course, but the length in distinct syntactic elements-- basically, the size of the parse tree. It may not be quite true that the shortest program is the least work to write, but it's close enough that you're better off aiming for the solid target of brevity than the fuzzy, nearby one of least work. Then the algorithm for language design becomes: look at a program and ask, is there any way to write this that's shorter?In practice, writing programs in an imaginary hundred-year language will work to varying degrees depending on how close you are to the core. Sort routines you can write now. But it would be hard to predict now what kinds of libraries might be needed in a hundred years. Presumably many libraries will be for domains that don't even exist yet. If SETI@home works, for example, we'll need libraries for communicating with aliens. Unless of course they are sufficiently advanced that they already communicate in XML.At the other extreme, I think you might be able to design the core language today. In fact, some might argue that it was already mostly designed in 1958.If the hundred year language were available today, would we want to program in it? One way to answer this question is to look back. If present-day programming languages had been available in 1960, would anyone have wanted to use them?In some ways, the answer is no. Languages today assume infrastructure that didn't exist in 1960. For example, a language in which indentation is significant, like Python, would not work very well on printer terminals. But putting such problems aside-- assuming, for example, that programs were all just written on paper-- would programmers of the 1960s have liked writing programs in the languages we use now?I think so. Some of the less imaginative ones, who had artifacts of early languages built into their ideas of what a program was, might have had trouble. (How can you manipulate data without doing pointer arithmetic? How can you implement flow charts without gotos?) But I think the smartest programmers would have had no trouble making the most of present-day languages, if they'd had them.If we had the hundred-year language now, it would at least make a great pseudocode. What about using it to write software? Since the hundred-year language will need to generate fast code for some applications, presumably it could generate code efficient enough to run acceptably well on our hardware. We might have to give more optimization advice than users in a hundred years, but it still might be a net win.Now we have two ideas that, if you combine them, suggest interesting possibilities: (1) the hundred-year language could, in principle, be designed today, and (2) such a language, if it existed, might be good to program in today. When you see these ideas laid out like that, it's hard not to think, why not try writing the hundred-year language now?When you're working on language design, I think it is good to have such a target and to keep it consciously in mind. When you learn to drive, one of the principles they teach you is to align the car not by lining up the hood with the stripes painted on the road, but by aiming at some point in the distance. Even if all you care about is what happens in the next ten feet, this is the right answer. I think we can and should do the same thing with programming languages. NotesI believe Lisp Machine Lisp was the first language to embody the principle that declarations (except those of dynamic variables) were merely optimization advice, and would not change the meaning of a correct program. Common Lisp seems to have been the first to state this explicitly.Thanks to Trevor Blackwell, Robert Morris, and Dan Giffin for reading drafts of this, and to Guido van Rossum, Jeremy Hylton, and the rest of the Python crew for inviting me to speak at PyCon. You'll find this essay and 14 others in Hackers & Painters.

---

_분석일: 2025. 11. 29._
_수집일: 2025. 11. 28._